{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1mWk0ndToqfnviNyvSHXK92LoZ4Uhzwpu",
      "authorship_tag": "ABX9TyOLu8U8HGs2R5Uxxaty58KL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veewy/AML/blob/main/AML_2)_Dataset_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¹ **Anti Money Laundering**  ðŸŒ¹"
      ],
      "metadata": {
        "id": "d2sRuJ5L7LqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data** **Import**"
      ],
      "metadata": {
        "id": "YzKB3ELs7Qob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from functools import partial\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from datetime import datetime\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from sklearn.metrics import f1_score, roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
        "from torch.nn.utils.rnn import pack_sequence"
      ],
      "metadata": {
        "id": "ArRSaxh-7Tc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-loading Intermediate Data"
      ],
      "metadata": {
        "id": "J5Urrs_M70pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reloading\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_df_test2 = \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/df_test2.parquet\"\n",
        "df_test2 = pd.read_parquet(path_df_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiylgMcR7Ta1",
        "outputId": "5326d929-3dd7-4fb7-d5cb-4dc335d1eaab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train / Test Split**"
      ],
      "metadata": {
        "id": "6cnZ5IBmUIxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down negative class_Full Dataset (with a.c. interaction)\n",
        "deal with imbalanced dataset\n"
      ],
      "metadata": {
        "id": "YbbIwRwx07nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train / validation / Test"
      ],
      "metadata": {
        "id": "NTtEKcCC1qSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the entire dataset (df_test2) into training (80%) and evaluation+test (20%) sets.\n",
        "\n",
        "X_events2 = df_test2['events']  # Features\n",
        "y_targets2 = df_test2['targets']  # Targets\n",
        "\n",
        "X_events_train2, X_events_eval_test2, y_targets_train2, y_targets_eval_test2 = train_test_split(X_events2, y_targets2, test_size=0.2, random_state=42)\n",
        "\n",
        "# create datasest\n",
        "#train_dataset2 = pd.concat([X_events_train, y_targets_train], axis=1)\n",
        "#eval_test_dataset2 = pd.concat([X_events_eval_test, y_targets_eval_test], axis=1)"
      ],
      "metadata": {
        "id": "SN5mmr_806Vc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#downsize only train set to handle data imbalance\n",
        "y_targets_train_pos2 = y_targets_train2[y_targets_train2.apply(sum) > 0]\n",
        "y_targets_train_neg2 = y_targets_train2[y_targets_train2.apply(sum) == 0]\n",
        "y_targets_train_neg_sampled2 = y_targets_train_neg2.sample(frac=0.105, random_state=42)\n",
        "y_targets_train_downsampled2 = pd.concat([y_targets_train_neg_sampled2, y_targets_train_pos2]).sample(frac=1, random_state=42)\n",
        "X_events_train_downsampled2 = X_events_train2[y_targets_train_downsampled2.index]\n",
        "\n",
        "# create datasest\n",
        "#train_dataset_downsampled2 = pd.concat([X_events_train_downsampled, y_targets_train_downsampled], axis=1)"
      ],
      "metadata": {
        "id": "k_CnyiupGccf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_targets_train_downsampled2), len(y_targets_eval_test2), y_targets_train_downsampled2.apply(sum).sum(), y_targets_eval_test2.apply(sum).sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SnxgfJDG7vU",
        "outputId": "d75bf3e9-400a-4f4f-a4a3-854694b17fa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77477, 171092, 15721, 4025)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "15721/77477"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGzKrlWBX7N5",
        "outputId": "faa4488f-a41d-4cf7-9b8a-f63e2e5f28b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2029118318984989"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "15721*5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6qJ4s4OXSSL",
        "outputId": "14a5c634-1b88-4264-8484-e117ea5ad7eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78605"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split portion of evaluation / test into evaluation (10%) and test (10%) sets.\n",
        "X_events_eval2, X_events_test2, y_targets_eval2, y_targets_test2 = train_test_split(X_events_eval_test2, y_targets_eval_test2, test_size=0.5, random_state=42)\n",
        "\n",
        "# create datasest\n",
        "#eval_dataset2 = pd.concat([X_events_eval, y_targets_eval], axis=1)\n",
        "#test_dataset2 = pd.concat([X_events_test, y_targets_test], axis=1)\n"
      ],
      "metadata": {
        "id": "_X_k99FYK6Ym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of eval and test\n",
        "len(y_targets_eval2), len(y_targets_test2), y_targets_eval2.apply(sum).sum(), y_targets_test2.apply(sum).sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZ85NDKL_XH",
        "outputId": "db017dd8-408f-4566-bfce-c71728dbaec8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85546, 85546, 2126, 1899)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset_downsampled2 = pd.concat([X_events_train_downsampled2, y_targets_train_downsampled2], axis=1)\n",
        "train_dataset2 = pd.concat([X_events_train2, y_targets_train2], axis=1)  # Full train dataset (no downsampling)\n",
        "eval_dataset2 = pd.concat([X_events_eval2, y_targets_eval2], axis=1)\n",
        "test_dataset2 = pd.concat([X_events_test2, y_targets_test2], axis=1)"
      ],
      "metadata": {
        "id": "z7IkjOkcShMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sizes for verification\n",
        "print(\"Train Dataset (Downsampled):\", train_dataset_downsampled2.shape)\n",
        "print(\"Train Dataset (Full):\", train_dataset2.shape)\n",
        "print(\"Eval Dataset:\", eval_dataset2.shape)\n",
        "print(\"Test Dataset:\", test_dataset2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Fuh7lKSUqB",
        "outputId": "9296cb81-68ee-4f1c-d5ee-0a131bdb4070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset (Downsampled): (77477, 2)\n",
            "Train Dataset (Full): (684368, 2)\n",
            "Eval Dataset: (85546, 2)\n",
            "Test Dataset: (85546, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataSet** / **DataLoader**"
      ],
      "metadata": {
        "id": "x-HvJFmnGJ-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmlDataset(Dataset):\n",
        "    def __init__(self, data, features):\n",
        "        \"\"\"\n",
        "        @param data: pdf whose index is monotonically increases from 0\n",
        "        @param features: list of features to be used in an event\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        psr_sample = self.data.iloc[index] #retrieves row at specified index from the data.\n",
        "        list_y = psr_sample[\"targets\"]\n",
        "        list_x = []\n",
        "\n",
        "        for event in psr_sample[\"events\"]:\n",
        "          x = [event[feature] for feature in self.features] #create X\n",
        "          list_x.append(x)\n",
        "\n",
        "        #Converts the extracted features (list_x) and targets (list_y) to NumPy arrays, Returns a tuple (x, y).\n",
        "        return np.array(list_x).astype(np.float32), np.array(list_y).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "sXyEoKiiGS_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select some related columns to be in training dataset (down neg. sample)\n",
        "aml_dataset_id_downs2 = AmlDataset(\n",
        "    train_dataset_downsampled2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "# define aml_dataloader --> for training dataset\n",
        "aml_dataloader_id_downs2 = DataLoader(\n",
        "    aml_dataset_id_downs2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False, # if we have GPU, set pin_memory=True\n",
        "    drop_last=False, # every sample in dataset is important, even if the final batch size varies. So will not drop it.\n",
        ")\n",
        "\n",
        "aml_dataloader_id_downs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMS-E-Q7J0Mc",
        "outputId": "80455a51-36fa-4291-df27-eca941b0242b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7c72c4e7fa90>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_dataset.to_parquet(\"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/eval_dataset.parquet\")\n"
      ],
      "metadata": {
        "id": "oMk8zseTWRTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define aml_eval_dataloader --> for evaluation\n",
        "aml_eval_dataset_id2 = AmlDataset(\n",
        "    eval_dataset2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "aml_eval_dataloader_id2 = DataLoader(\n",
        "    aml_eval_dataset_id2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False,\n",
        "    #collate_fn=lambda batch: custom_collate_fn(batch, max_history_length=50)  # Use the custom collate function\n",
        ")\n",
        "\n",
        "aml_eval_dataloader_id2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-mZwBwxJ0KU",
        "outputId": "be31fbd7-5e25-4668-ce2c-4c0598d828a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7c72c4e7d690>"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test_dataset.to_parquet(\"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/test_dataset.parquet\")\n"
      ],
      "metadata": {
        "id": "wuyZ0_FPWXFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define aml_eval_dataloader --> for testing dataset\n",
        "aml_test_dataset_id2 = AmlDataset(\n",
        "    test_dataset2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "aml_test_dataloader_id2 = DataLoader(\n",
        "    aml_test_dataset_id2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False,\n",
        "    #collate_fn=lambda batch: custom_collate_fn(batch, max_history_length=50)  # Use the custom collate function\n",
        ")\n",
        "\n",
        "aml_test_dataloader_id2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OarG8t2MKATT",
        "outputId": "e71e4808-3b12-4ec9-b916-577e221739e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7c72c4e7e8c0>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save path of Dataset/DataLoader"
      ],
      "metadata": {
        "id": "4uRd8cTifWIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for training part\n",
        "\n",
        "torch.save(aml_dataset_id_downs2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_dataset_id_downs2.pth\")\n",
        "torch.save(aml_dataloader_id_downs2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_dataloader_id_downs2.pth\")"
      ],
      "metadata": {
        "id": "a221P0BDJsoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for evaluating part\n",
        "\n",
        "torch.save(aml_eval_dataset_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_eval_dataset_id2.pth\")\n",
        "torch.save(aml_eval_dataloader_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_eval_dataloader_id2.pth\")"
      ],
      "metadata": {
        "id": "hRO9AN2ZWM8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing part\n",
        "\n",
        "torch.save(aml_test_dataset_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_test_dataset_id2.pth\")\n",
        "torch.save(aml_test_dataloader_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_test_dataloader_id2.pth\")"
      ],
      "metadata": {
        "id": "9i6Ym19dWNbL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}