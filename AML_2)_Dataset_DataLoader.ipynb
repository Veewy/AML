{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "mount_file_id": "1mWk0ndToqfnviNyvSHXK92LoZ4Uhzwpu",
      "authorship_tag": "ABX9TyMSDEPwpBBXR/zG0r1sjh3H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veewy/AML/blob/main/AML_2)_Dataset_DataLoader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŒ¹ **Anti Money Laundering**  ðŸŒ¹"
      ],
      "metadata": {
        "id": "d2sRuJ5L7LqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data** **Import**"
      ],
      "metadata": {
        "id": "YzKB3ELs7Qob"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np                                    # for numerical operations and array manipulations\n",
        "import torch                                          # core PyTorch library\n",
        "from torch.utils.data import Dataset, DataLoader      # for creating custom datasets and dataloaders\n",
        "from sklearn.model_selection import train_test_split  # split datasets into training and testing subsets"
      ],
      "metadata": {
        "id": "ArRSaxh-7Tc4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Re-loading Intermediate Data"
      ],
      "metadata": {
        "id": "J5Urrs_M70pb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reloading data from previous step\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "path_df_test2 = \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/df_test2.parquet\"\n",
        "df_test2 = pd.read_parquet(path_df_test2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XiylgMcR7Ta1",
        "outputId": "1f3261fa-154a-4758-99b6-c22d03642d66"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train / Test Split**"
      ],
      "metadata": {
        "id": "6cnZ5IBmUIxv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Down negative class_Full Dataset (with a.c. interaction)\n",
        "deal with imbalanced dataset\n"
      ],
      "metadata": {
        "id": "YbbIwRwx07nK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train / validation / Test"
      ],
      "metadata": {
        "id": "NTtEKcCC1qSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split the entire dataset (df_test2) into training (80%) and evaluation+test (20%) sets.\n",
        "\n",
        "X_events2 = df_test2['events']  # Features\n",
        "y_targets2 = df_test2['targets']  # Targets\n",
        "\n",
        "X_events_train2, X_events_eval_test2, y_targets_train2, y_targets_eval_test2 = train_test_split(X_events2, y_targets2, test_size=0.2, random_state=42)\n"
      ],
      "metadata": {
        "id": "SN5mmr_806Vc"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# downsizing only train set to handle data imbalance\n",
        "y_targets_train_pos2 = y_targets_train2[y_targets_train2.apply(sum) > 0]\n",
        "y_targets_train_neg2 = y_targets_train2[y_targets_train2.apply(sum) == 0]\n",
        "y_targets_train_neg_sampled2 = y_targets_train_neg2.sample(frac=0.105, random_state=42)\n",
        "y_targets_train_downsampled2 = pd.concat([y_targets_train_neg_sampled2, y_targets_train_pos2]).sample(frac=1, random_state=42)\n",
        "X_events_train_downsampled2 = X_events_train2[y_targets_train_downsampled2.index]\n"
      ],
      "metadata": {
        "id": "k_CnyiupGccf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_targets_train_downsampled2), len(y_targets_eval_test2), y_targets_train_downsampled2.apply(sum).sum(), y_targets_eval_test2.apply(sum).sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SnxgfJDG7vU",
        "outputId": "6560161b-48d1-4e57-87d9-a5774d56b163"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(77477, 171092, 15721, 4025)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "15721/77477"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGzKrlWBX7N5",
        "outputId": "1e7551e3-59c3-4241-8801-9db897328286"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2029118318984989"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split portion of evaluation:test into evaluation (10%) and test (10%) sets.\n",
        "X_events_eval2, X_events_test2, y_targets_eval2, y_targets_test2 = train_test_split(X_events_eval_test2, y_targets_eval_test2, test_size=0.5, random_state=42)\n"
      ],
      "metadata": {
        "id": "_X_k99FYK6Ym"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check size of eval and test\n",
        "len(y_targets_eval2), len(y_targets_test2), y_targets_eval2.apply(sum).sum(), y_targets_test2.apply(sum).sum()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OZ85NDKL_XH",
        "outputId": "a479f68e-ba50-4443-8d24-52e8535621ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(85546, 85546, 2126, 1899)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset_downsampled2 = pd.concat([X_events_train_downsampled2, y_targets_train_downsampled2], axis=1)\n",
        "train_dataset2 = pd.concat([X_events_train2, y_targets_train2], axis=1)  # Full train dataset (no downsampling)\n",
        "eval_dataset2 = pd.concat([X_events_eval2, y_targets_eval2], axis=1)\n",
        "test_dataset2 = pd.concat([X_events_test2, y_targets_test2], axis=1)"
      ],
      "metadata": {
        "id": "z7IkjOkcShMX"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print sizes for verification\n",
        "print(\"Train Dataset (Downsampled):\", train_dataset_downsampled2.shape)\n",
        "print(\"Train Dataset (Full):\", train_dataset2.shape)\n",
        "print(\"Eval Dataset:\", eval_dataset2.shape)\n",
        "print(\"Test Dataset:\", test_dataset2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "89Fuh7lKSUqB",
        "outputId": "5b250be1-9f37-48c3-8df2-0630684a38f6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Dataset (Downsampled): (77477, 2)\n",
            "Train Dataset (Full): (684368, 2)\n",
            "Eval Dataset: (85546, 2)\n",
            "Test Dataset: (85546, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataSet** / **DataLoader**"
      ],
      "metadata": {
        "id": "x-HvJFmnGJ-g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AmlDataset(Dataset):\n",
        "    def __init__(self, data, features):\n",
        "        \"\"\"\n",
        "        @param data: pdf whose index is monotonically increases from 0\n",
        "        @param features: list of features to be used in an event\n",
        "        \"\"\"\n",
        "        self.data = data\n",
        "        self.features = features\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        psr_sample = self.data.iloc[index] #retrieves row at specified index from the data.\n",
        "        list_y = psr_sample[\"targets\"]\n",
        "        list_x = []\n",
        "\n",
        "        for event in psr_sample[\"events\"]:\n",
        "          x = [event[feature] for feature in self.features] #create X\n",
        "          list_x.append(x)\n",
        "\n",
        "        #Converts the extracted features (list_x) and targets (list_y) to NumPy arrays, Returns a tuple (x, y).\n",
        "        return np.array(list_x).astype(np.float32), np.array(list_y).astype(np.float32)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "metadata": {
        "id": "sXyEoKiiGS_9"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# select some related columns to be in training dataset (down neg. sample)\n",
        "aml_dataset_id_downs2 = AmlDataset(\n",
        "    train_dataset_downsampled2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "# define aml_dataloader --> for training dataset\n",
        "aml_dataloader_id_downs2 = DataLoader(\n",
        "    aml_dataset_id_downs2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    pin_memory=False, # if we have GPU, set pin_memory=True\n",
        "    drop_last=False, # every sample in dataset is important, even if the final batch size varies. So will not drop it.\n",
        ")\n",
        "\n",
        "aml_dataloader_id_downs2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMS-E-Q7J0Mc",
        "outputId": "73e52d09-d2b0-4833-fbcf-3cf66ae97628"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7bf4f2ad4970>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# eval_dataset.to_parquet(\"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/eval_dataset.parquet\")\n"
      ],
      "metadata": {
        "id": "oMk8zseTWRTo"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define aml_eval_dataloader --> for evaluation\n",
        "aml_eval_dataset_id2 = AmlDataset(\n",
        "    eval_dataset2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "aml_eval_dataloader_id2 = DataLoader(\n",
        "    aml_eval_dataset_id2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False,\n",
        "    #collate_fn=lambda batch: custom_collate_fn(batch, max_history_length=50)  # Use the custom collate function\n",
        ")\n",
        "\n",
        "aml_eval_dataloader_id2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-mZwBwxJ0KU",
        "outputId": "8f97a7d1-fb11-47e2-8234-75f5e81a56ba"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7bf4f2ad4ca0>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define aml_eval_dataloader --> for testing dataset\n",
        "aml_test_dataset_id2 = AmlDataset(\n",
        "    test_dataset2,\n",
        "     ['account_encoded', 'day_of_week_encoded', 'hour', 'transaction_type_encoded','account_interaction_encoded',\n",
        "      'payment_currency_encoded', 'received_currency_encoded',\n",
        "      'sender_bank_location_encoded', 'receiver_bank_location_encoded',\n",
        "      'time_interval_normalized', 'amount_normalized'\n",
        "      ]\n",
        "    )\n",
        "\n",
        "aml_test_dataloader_id2 = DataLoader(\n",
        "    aml_test_dataset_id2,\n",
        "    batch_size= None,\n",
        "    shuffle=False,\n",
        "    num_workers=0,\n",
        "    drop_last=False,\n",
        "    #collate_fn=lambda batch: custom_collate_fn(batch, max_history_length=50)  # Use the custom collate function\n",
        ")\n",
        "\n",
        "aml_test_dataloader_id2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OarG8t2MKATT",
        "outputId": "ba0cfaf5-4274-40de-99e0-ab2b42871d4a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7bf4f2ad7a60>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save path of Dataset/DataLoader"
      ],
      "metadata": {
        "id": "4uRd8cTifWIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for training part\n",
        "\n",
        "torch.save(aml_dataset_id_downs2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_dataset_id_downs2.pth\")\n",
        "torch.save(aml_dataloader_id_downs2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_dataloader_id_downs2.pth\")"
      ],
      "metadata": {
        "id": "a221P0BDJsoM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for evaluating part\n",
        "\n",
        "torch.save(aml_eval_dataset_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_eval_dataset_id2.pth\")\n",
        "torch.save(aml_eval_dataloader_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_eval_dataloader_id2.pth\")"
      ],
      "metadata": {
        "id": "hRO9AN2ZWM8t"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for testing part\n",
        "\n",
        "torch.save(aml_test_dataset_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_test_dataset_id2.pth\")\n",
        "torch.save(aml_test_dataloader_id2, \"/content/drive/MyDrive/DE/Master_Degree/3rd_Semester/Colab_Notebook/AML_file/aml_test_dataloader_id2.pth\")"
      ],
      "metadata": {
        "id": "9i6Ym19dWNbL"
      },
      "execution_count": 21,
      "outputs": []
    }
  ]
}